\chapter{Seeking Interoperability} \label{ch:interoperability}

Having ontological tools for describing scientific workflows makes it possible
to consider how workflow management systems might interoperate. This could
include executing workflow campaigns that combine and distribute workflows
based solely on the descriptions of the workflows themselves. Interoperable
workflow management systems would be extremely valuable to the scientific
computing community because of the need to solve increasingly interdisciplinary
problems, often using highly componentized domain-specific workflow tools while
leveraging exotic architectures.

This chapter explores various aspects of interoperability, including its
important relationship to interchangeability, and how differences in workflows
and subworkflows can be identified ontologically and be shared. Finally, as an
example of what ontological tools can offer, a new system for exchanging
workflows, data, and metadata that is built on ontological principles is
discussed in the context of distributed workflow provenance.

\section{Interoperability}
\baseInclude{pubs/workflows-paper/src/experience-interop}

\baseInclude{pubs/workflows-paper/src/interoperability}

\subsection{Interoperable Versus Interchangeable}

It is important to understand the distinction between interoperable and
interchangeable. Interoperable describes systems that work together by
exchanging information, and interchangeable describes things that can be
substituted exactly for each other.

Interchangeable workflows would mean that any workflow management system could
interpret the workflow description for any other workflow management system.
Interchangeable workflow descriptions would be as easy to swap as components in
computers or parts on two cars of the same model. On the other hand,
interoperability for workflows would mean that the workflow management systems
that execute workflows could exchange information and operate together to
ensure complete execution of the workflows.

Requiring components to be interchangeable is a stronger constraint on systems
than requiring that they be interoperable. The examples of \S
\ref{workflow-ont-examples} can be used to illustrate this. Consider the move
workflow of \S \ref{move-workflow}. If two systems supported interchangeable
workflow descriptions, then both systems would be required to fully execute the
original workflows. Interoperable workflow systems would instead only be
required to execute the pieces they could - perhaps \#moveAction - while leaving
the rest to partnering sytems.

The conceptual neutron scattering workflow shown in Figure
\ref{neutron-workflow} depicts a system of interoperable workflow management
systems. This is evident because no one system could be reasonably expected to
do all the proposed tasks. Modeling and simulation tools to perform density
functional theory calculations are not the same as tools used to capture,
reduce, and curate experimental data. (The differences in workflow management tools
across these broad domains are discussed extensively in Chapter
\ref{ch:introduction}.) Real user workflows that are executed to realize this
conceptual workflow link together many different workflow tools, scripts, and
packages that translate, shape, convert, and otherwise manipulate input and
output data between the steps. Each litle piece of software does its part to
exchange and transmute the data in such a way that the other pieces can use it,
and, therefore, creates an interoperable ecosystem.

This chapter explores the lesser of these two, interoperability, because there
are immediate use cases for describing workflows in a common way and
distributing the execution across multiple systems. These needs exist in
science because, as in the previous example, no workflow management system
exists that can handle every subject. Many workflow systems exist that cannot be
changed but must still be used. On the contrary, interchangeable workflows
would require that all the systems be rebuilt to meet ``interchangeability
standards,'' which would increase the development burden and thereby decrease
the probability of the developers supporting it.

\subsection{Practical Hangups}
\label{practicality}

Practical systems exhibit many properties not found in theoretical systems that
limit or prohibit the ability of developers to provide any of the four
properties listed previously. This could be as conceptually simple as language
differences that impede communication, challenges with backwards compatibility,
missing features in standard libraries, or as complex as deep, subtle design
differences that are not easily rectified without translation or
reimplementation. Theoretically these challenges can all be rectified in a few
pages of instructive text. However, an old economic adage applies:
``There ain't no such thing as a free lunch.''\footnote{Physicists quickly recognize this as a concise
restatement of the laws of thermodynamics. Arguably, an experimental
thermodynamicist would never get bored applying their trade to the study of
programmers.} That is, someone must do the work to address these issues in real
systems.

Consider some of these issue that can be quickly and easily identified between
Eclipse ICE versions 2.0 and 3.0. An existing system, such as Eclipse ICE 2.0,
likely has a large number of features, requirements, bug fixes, and ---being completely
honest--- ``hacks'' that were necessary to acheive desired functionality. These
could be very obvious, such as the fact that Eclipse ICE 2.0 does not use the Resource
Description Framework (RDF) whereas version 3.0 does. Several examples of these
types of problems are discussed next to illustrate the nature of
compability issues between systems, including systems developed by the same authors.

\subsubsection{Changes in Data Structure Design}

The aforementioned example of ICE 3.0 using RDF to describe its data structures
is an obvious example of a difference between it and a legacy system such as ICE
2.0. At a design level, this means that ICE 3.0 has data structures that are
described completely declaratively and ontologically in RDF, the RDF Schema
languages (RDFS), or the Web Ontology Language (OWL). The definitions of the
data structures are defined in OWL, which is itself defined in RDF, and
instances of these data structures are defined in RDF as well. The ontology
exists in one OWL-RDF file, and the instances exist in their own files that
import the base ontology.

ICE 3.0 not only has different data structures but also uses uses a third-party
library, Apache Jena, to implement these data structures and provide useful
services, such as mapping to HTTP/HTTPS servers or reading and writing to
disks. ICE 2.0 used a custom implementation of its data structures, which were
originally transcoded from UML to Java and updated thereafter by hand.

Other differences are far more subtle and demanding in their detail. Data
structures in ICE 2.0 manage their own notifications. Observers can
\textit{register} as listeners to a data structure in ICE 2.0, and that data
structure will notify the observer when it changes. This is a very low-level
implementation of the observer pattern that was designed
to i) facilitate live updates in user interfaces, and ii) to manage dependencies
between data structures asynchronously. The latter case made it possible for a
data structure to change its own value in response to a change made to a second
data structure that was observing what was modified in the user interface. Both
data structures would then asynchronously update their own observers and the
user interface would finally update dynamically. Such a complicated case as
this is commonly used in ICE 2.0, and live updates to the user interface happen
within every workflow task.

RDF and OWL models in Apache Jena (and thus ICE 3.0) are available only at the
highest level of the data model, and they are very coarse grained: They say only
that the model changed, not what data structure in the model changed. This is a
completely valid way to handle update notifications, but in contrast to the
model of ICE 2.0 it requires a full reload of the data structures by clients,
including user interfaces. It has the distinct advantage of using far less
resources than the ICE 2.0 model, namely that it does not launch separate
threads for each update, but it comes at the possible cost of an expensive
reload with every significant update. However, there are numerous strategies
for mitigating the performance cost of a large reload.

The implications of a subtle change like this can be far reaching. For example,
ICE 2.0 workflows that execute in ICE 3.0 will need to be broken into smaller
pieces that divide any regions of dependency management into distinct steps.
Alternatively, a decorator pattern could be implemented around the model to
capture function calls and thereby identify which elements of the data model
updated. This would, in effect, acheive the properties of the ICE 2.0 model
using the new model provided through Apache Jena.

\section{Using the Ontology for Interoperability}

The ontology in Chapter \ref{ch:workflow-ontology} is shown by example to cover
a range of workflows. Four of the five examples are workflows that can be
executed, with two of the examples being executed from significantly different
workflow engines, Eclipse ICE and Pegasus. Suppose that these four workflows
were to be strung together into a single workflow that accomplishes the
following:
\begin{enumerate}
  \item Gathers some data using a threshold limiter
  \item Moves the data to a work directory
  \item Splits the data into smaller files
  \item Performs simulations using the data and some human feedback as input
  \item Moves the data and results back to the original directory
  \item Loops over all the files in the directory, deleting the unneeded ones
\end{enumerate}
This combined workflow campaign stiches some of the workflows together directly
(steps 2--5), and splits the cycling and looping workflow to stage data.
Conceptually, this is simple, but in practice it may require years of work.

The remaining question of this work is whether or not the workflow ontology
simplifies the execution of a workflow campaign, such as that described by
this example; that is, does it help the workflow management systems charged with
executing these workflows interoperate? The answer is that, in principle, it
does because i) the ontology supports nested workflows by domain and range
restrictions on workflow description and task and ii) it provides a formal,
common language for describing these different workflows that could not
otherwise be described collectively. Together these two properties mean that
any combination of workflows can be walked as a tree, split, and (re)-combined
so long as the workflows (ontology instances) can understood by the ``client''
workflow management systems.

An alternative answer to this question is from the practical perspective of \S
\ref{practicality}: In so far as the workflow ontology can describe workflows
for different systems perfectly, it only serves to prove how different they are.
Thus, practically, the answer is also that the ontology does not facilitate
greater interoperability for workflows because it does not address the
differences across workflow management systems, including the absence of any
particular application programming interface. This is a perspective that couples
the execution of a workflow problem to the workflow management system made to
solve it in the first place.

How can these two positions be rectified? On the one hand, there is a promise of
improved interoperability provided by machine-readable formality and rigor. On
the other, there is the very practical point that most workflow management
systems today are in closed ecosystems.

The answer is that the workflow ontology should not be used for anything other
than what it is: a means to formally describe, capture, and share information
about workflows. It can fix only problems it was developed to solve. However, it
can serve as an important decision-making tool and virtual road map for
developers, designers, users, and project planners, while still remaining
something that can be used by a select few tools to translate, decompose,
catalogue, and investigate workflows. The steps for using the workflow ontology
effectively is a graded approach. The following sections provide instructions
for several use cases of the workflow ontology applied to practical workflow
problems.

\subsection{Typing Workflows}

Chapter \ref{ch:introduction} includes details for multiple distinct types of
workflows, and the workflow ontology can be used to categorize any workflow into
these types. First, create a model of the workflow in question and visualize it
using Graphviz. The type of workflow can be determined as follows:
\begin{itemize}
  \item If the workflow is linear, with no loops, cycles, and directed
  dependencies, then the workflow is a high-throughput workflow based on a
  directed acyclic graph.
  \item If the workflow requires conditional behavior, and external (possibly
  human) feedback and exhibits state changes, then it may be a
  modeling and simulation or similar type of workflow.
  \item If the workflow requires only tasks with few concrete parameters,
  properties, or anything that would make it executable, then it is a conceptual
  workflow.
  \item If the workflow appears to have an embedded workflow that is executed
  iteratively or repeatedly in a loop or threshold cycle, then it may be a
  testing, optimization, or analysis workflow.
\end{itemize}

\subsection{Comparing Workflows}

Workflows can be compared directly using the workflow ontology. This is useful
for workflows typed as described in the previous section or for workflows that
are very similar to each other but described in separate and perhaps confusing
description languages. First, create a model of two or more workflows using the
workflow ontology, starting with the workflow description (root), tasks,
actions, and task dependencies. If a precise relationship between the workflows
(or lack thereof) cannot be determined because the tasks are too similar,
continue to model the properties and, if necessary, any state changes in the
model.

This may be done either manually or by using RDF tools such as Apache Jena.
Ontology and RDF model mergers are common features of these tools since the
semantic underpinnings of RDF make it possible to compare graphs directly. The
knowledge to use the ontology to determine whether or not two workflow models
are similar is embedded in the graph structure itself. Nodes
(tasks, actions, etc.) can be compared as the graphs are walked, either to
neighbors at the same level or to different levels across the graphs.

\subsection{Building Campaigns}

Workflow campaigns can be built in a straightforward fashion once workflows have
been typed, compared, and thoroughly analyzed. For workflows that can actually
be executed, a new root-level workflow description must be created. Each
distinct workflow description that will be part of the campaign can be added
into the new parent workflow description through the ontology's
``executes'' object property, which can be applied workflow descriptions as
well as tasks. This produces a single workflow description with all subservient
workflow descriptions in the same graph.

Distributed workflow campaigns can be produced by using the ``hasHost'' property
for workflow descriptions and tasks to indicate that the selected portion should
be executed on the specified host.

Alternatively, workflow campaigns can be created by simply subdividing
workflows to create subworkflows that are executed on their own resources. This
process works by identifying distinct branches of a workflow description and
applying the ``hasHost'' the tasks of those branches. 

\section{Data Management, Provenance, and Workflows Distributed on the
Blockchain}

\todo{Important realization with data in an open world is how do you manage it?
Actors must be free to\ldots
*Describe what they have
*Store bulk data
*Share what they have}

This section describes the Basic Artifcat Tracking System (BATS) that
was developed as part of this thesis work to address data management,
provenance, and workflows. This is an example of how new technologies that solve
longstanding problems can benefit from leveraging ontological principles and
especially how this can be applied to the problems of workflow provenance.

\baseInclude{pubs/billings-workflows-blockchain/content}

\section{Summary}

This chapter discusses various considerations for interoperability for
scientific workflows from the perspective of the workflow ontology developed as
part of this work. It illustrates a problem faced by many new tools but
especially this one: Promising opportunities would be available if tighter
integration existed within the tooling stack. However, this situation limits
only the usefulness of the workflow ontology as a programming tool; it still
functions well as a tool for reasoning and discussing scientific workflows. 

This chapter also examines a real-world problem where the workflow ontology is
proving useful in sharing and distributing provenance information across a
permissioned distributed ledger. This example is provided to illustrate how new
technologies can leverage the workflow ontology from the outset to solve new
problems. 

The next chapter considers these findings and other aspects of this work to
draw conclusions about the value of ontological tools in the scientific workflow
space.