\chapter{Introduction} \label{ch:introduction}

Many civilizations tell an origin story for the diversity of human language. The
common thread in different versions of this story is that humanity
originally spoke a single language and united together to build a tower so tall
it could reach heaven. Some versions say the builders used blocks of
a common size, while others say the builders used timbers of a common
length. The end is the same in most versions: When God learns of the
tower, he punishes the builders by confusing them and then spreads them across
the world. The tower is left unfinished, heaven is left untouched, and the
builders are left speaking different languages.

Through an amount of research effort roughly equal to the work required
to build a tower that can reach heaven, modern linguistics has demonstrated the
low likelihood of an original, single human language. However, the insights gained
about the nature of language, human anatomy, learning, and neuroscience from
this effort were very valuable in their own right because of what they enabled or
revealed in other research efforts.

As complex as human language may be, computer science may represent the ultimate
test of our ability to study diverse ecosystems with new languages and tools
under constant, continuous development. Language and tool diversity is a
benefit to computing because each new language or tool is designed purposely to
solve a new problem, or to solve an old problem in a new way. This allows for
the entire technology stack to  be layered, optimized, and deployed in ways
specifically designed to exploit favorable conditions in complex systems. Case
in point, older programming languages such as Fortran and Kobol did not lose
popularity because of divine intervention. They lost popularity because of
economic forces that drove the development and adoption of more portable and
expressive system languages, such as C. Fortran and Kobol are still used in
places where they make sense, including high-performance computing and finance,
but better tools are used where Fortran and Kobol are less than optimal.

One important class of programming languages and tools includes those that
can be combined with data to streamline and, in many cases, automate the
execution of tasks and processes. The major advantage of these tools is that
they make previously cumbersome activities repeatable and highly efficient.
This class solves \textit{workflow problems} and is especially noteworthy
because of the poorly understood panoply of tools found in this space.

\section{Specific Contributions}

This work examines workflow problems and assocciated technology under two
assumptions that can be seen in parallel with broader
computing ecosystems. Specifically, it assumes that (i) there are no preferred
universal languages or tools and (ii) lack of standardization
in software solutions is common because it is beneficial. Based
on these assumptions, this work shows that 
\begin{itemize}
  \item the workflow technology space is well covered by different types
  of systems,
  \item an ontological treatment can be used to create a classification scheme
  for workflows, and
  \item this scheme can be used for next-generation challenges such as system
  interoperability and decision-making.
\end{itemize}

This chapter provides a thorough introduction of the workflow problem space as
well as some philosopical background to prepare the reader. Chapter
\ref{ch:eclipse-ice} introduces the Eclipse Integrated Computational Environment
(Eclipse ICE), which acted as the primary model and served as a starting point
for much of the ontological and technical work. Chapter \ref{ch:ontologies}
reviews ontologies and associated tools, while Chapter
\ref{ch:workflow-ontology} presents an ontological model relevant to workflows,
including multiple examples of its application. Interoperability viewed through
an ontological lens is discussed \ref{ch:interoperability}. Chapter
\ref{ch:interoperability} also details a new model for data management and provenance
capture for scientific workflows that embodies the principles shared herein. A
final summary and discussion of the value of and opportunities for future work
are presented in chapter \ref{ch:conclusions}.

\subsubsection{Content sources}

The content in this document is largely based on separately published papers
that were collected, expanded, and edited for the purposes of better supporting
the argumentative stance of a dissertation and meeting the formatting
requirements. The introductory text in this chapter is largely based on
work published previously in the Open Source Supercomputing Workshop
\cite{billings_toward_2017}. The content of Chapter \ref{ch:eclipse-ice} was
adapted from a manuscript in the journal \textit{Software X}
\cite{billings_eclipse_2017}. The data management system in Chapter
\ref{ch:interoperability} includes work presented as an invited talk at the First
International Workshop on Practical Reproducible Evaluation of Computer
Systems, with additional content on new work and the \textit{Basic Artifact
Tracking System (BATS)}, which is in production use. Additional content has
been adapted from slides presented at international conferences and workshops,
and committee meetings.

The ontological and classification work presented in Chapters
\ref{ch:ontologies} and \ref{ch:interoperability} is completely new, and at
time of this writing has not been published in manuscript form in
workshop, conference, or journal publications. However, the full source of the
ontologies and code has been made available on Github.com in the Eclipse ICE repository,
\cite{billings_ice}.

\todo{FIX the Eclipse ICE reference needs to be updated to
point to Software X}

\section{Workflows}

\baseInclude{pubs/workflows-paper/src/introduction}
\baseInclude{pubs/workflows-paper/src/workflows-review}
\baseInclude{pubs/workflows-paper/src/experience}
\baseInclude{pubs/workflows-paper/src/common}
\baseInclude{pubs/workflows-paper/src/buildingblocks}

\section{Summary}

The previous sections illustrate the complexity and diversity of workflow
technologies. Having amassed such data on the topic, it is tempting to develop
a new or adopt an existing definition of ``workflow'' and ``workflow system.''
However, settling on a single, simple definition has not worked well in the past
for a wide enough cross section of the community to meet future needs as workflows
begin to integrate experimental, computational, and analytical processes at
larger scales. Even more rigorous methods that attempt to create relevant
taxonomies are restricted to a single community, such as grid workflows in the
case of Yu and Buyya.

It is highly desirable to develop a deeper understanding of the similarities and
differences between workflows and related systems for several reasons. First,
avoiding unnecessary duplication can be avoided and gaining a greater
understanding should help with decision-making and resource allocation. Second,
a deeper understanding may make it possible to do new, highly desirable things
with workflow management systems. Finally, such understanding may reveal new
ways to improve or use related technologies including data management, machine learning, and
artificial intelligence.

Ontologies are efficient tools for gaining such an understanding as they
can formally catalog all of the different properties relevant to gaining
knowledge in a given topical area. This can be done in both human and machine
readable ways. The following chapters provide just such an analysis. However,
before turning to ontological considerations, and in an effort to better
understand the origins of the questions at the core of this thesis, it is
important to look at an interesting and somewhat unique workflow management
system: Eclipse ICE.

