\chapter{Introduction} \label{ch:introduction}
\todo{Need a statement somewhere that more work has gone into systems than
workflows per se.}

Many civilizations tell an origin story for the diversity of human language. The
common thread in different versions of this story is that humanity
originally spoke a single language and united together to build a tower so tall
that it could reach heaven. Some versions say that the builders used blocks of
a common size, while others say that the builders used timbers of a common
length. The end is the same in most versions: When God learns of the
tower, he punishes the builders by confusing them, and then spreads them across
the world. The tower is left unfinished, heaven is left untouched, and the
builders are left speaking different languages.

Through an amount of research effort roughly equally to the work required
to build a sky-high tower, modern linguistics has demonstrated the low
likelihood of an original, single human language. However, the insights gained
about the nature of language, human anatomy, learning and neuroscience from this
effort were very valuable in their own right because of what they enabled or
revealed in other research efforts.

As complex as human language may be, computer science may represent the ultimate
test of our ability to study diverse ecosystems with new languages and tools
under constant, continuous development. Language and tool diversity is a
benefit to computing because each new language or tool is designed purposely to
solve a new problem, or to solve an old problem in a new way. This allows for
the entire technology stack to  be layered, optimized, and deployed in ways
specifically designed to exploit favorable conditions in complex systems. Case
in point, older programming languages such as Fortran and Kobol did not lose
popularity because of divine intervention. They lost popularity because of
economic forces that drove the development and adoption of more portable and
expressive system languages, such as C. Fortran and Kobol are still used in
places where they make sense, including high performance computing and finance,
but better tools are used where Fortran and Kobol are less than optimal.

One important class of programming languages and tools includes those that
can be combined with data to streamline and, in many cases, automate the
execution of tasks and processes. The major advantage of these tools is that
they make previously cumbersome activities repeatable and highly efficient.
This class solves \textit{workflow problems}, and is especially noteworthy
because of the poorly understood panoply of tools found in this space.

This work examines workflow problems and assocciated technology under two
assumptions that can be seen in parallel with broader
computing ecosystem. Specifically it considers that 1) there are no preferred
universal languages or tools, and 2) a lack of standardization
in software solutions is common because it is beneficial. Based
on these assumptions, this work shows that 
\begin{itemize}
  \item the workflow technology space is well covered by different types
  of systems,
  \item that an ontological treatment can be used to create a map of
  workflows and workflow management systems,
  \item and that this map can be used for next generation
  challenges such system interoperability and decision making.
\end{itemize}

The present chapter provides a thorough introduction of the workflow problem
space as well as some philosopical background to prepare the reader. Chapter
\ref{ch:ontologies} discusses ontologies, associated tools, and ontological
models relevant to workflows. Interoperability enabled solely through
ontological considerations is presented in chapter
\ref{ch:interoperability}. Chapter \ref{ch:eclipse-ice} introduces the
Eclipse Integrated Computational Environment (Eclipse ICE), which acted as the
primary model and served as a starting point for much of the
ontological and technical work. Chapter \ref{ch:blockchain} details a new model
data management and provenance capture for scientific workflows that embodies
the principles shared herein. A final summary and discussion of the
value of and opportunities for future work are presented in chapter
\ref{ch:conclusions}.

\subsubsection{Content sources}

The content in this document is largely based on separately published papers
that were collected, expanded, and edited for the purposes of better supporting
the argumentative stance of a thesis, and the formatting requirements of
the graduate school. The introductory text in this chapter is largely based on
work published previously in the Open Source Supercomputing workshop,
\cite{billings_toward_2017}. The content of chapter \ref{ch:eclipse-ice} was
adapted from a manuscript in the journal Software X,
\cite{billings_eclipse_2017}. The data management system in chapter
\ref{ch:blockchain} includes work presented as an invited talk at the First
International Workshop on Practical Reproducible Evaluation of Computer Systems
(P-RECS'18) with additional content on new work and the software system, the
\textit{Basic Artifact Tracking System (BATS)}, which has entered production
use. Additional content has been adapted from slides presented at international
conferences and workshops, as well as committee meetings.

The ontological and classification work presented in chapters
\ref{ch:ontologies} and \ref{ch:interoperability} is completely new, and at
time of this writing has not been published in manuscript form in
workshop, conference, or journal. However, the full source of the ontologies
and code has been made available on Github.com in the Eclipse ICE repository,
\cite{billings_ice}.

\todo{FIX the Eclipse ICE reference needs to be updated to
point to Software X}
\todo{the GitHub Eclipse ICE reference needs to be checked}

\section{Workflows}

\todo{Review CBB section}
\textit{The role that CBBs can play are at a system level. The section can be
reframed based on that.}

\baseInclude{pubs/workflows-paper/src/introduction}
\baseInclude{pubs/workflows-paper/src/workflows-review}
\baseInclude{pubs/workflows-paper/src/experience}
\baseInclude{pubs/workflows-paper/src/common}
\baseInclude{pubs/workflows-paper/src/buildingblocks}

\section{Summary}

The previous sections illustrate the complexity and diversity of workflow
technologies. Having amassed such data on the topic, it is tempting to develop
a new or adopt an existing definition of ``workflow'' and ``workflow system.''
However, settling on a single, simple definition has not worked well in the past
for a wide enough cross section of the community to meet future needs as workflows
begin to integrate experimental, computational, and analytical processes at
larger scales. Even more rigorous methods that attempt to create relevant
taxonomies are restricted to a single community, such as grid workflows in the
case of Yu and Buyya.

It is highly desirable to develop a deeper understanding of the similarities and
differences between workflows and related systems for several reasons. First, if
unnecessary duplication can be avoided and a greater understanding gained, they
should be to help with decision making and resource allocation. Second, a
deeper understanding may make it possible to do new, highly desirable things
with workflow management systems. Finally, it may reveal new ways to improve or
use related technologies including data management, machine learning, and
artificial intelligence.

Ontologies are efficient tools for gaining such an understanding as they
can formally catalog all of the different properties relevant to gaining
knowledge in a given topical area. This can be done in both human and machine
readable ways. The following chapters provide just such an analysis. However,
before turning to ontological considerations, and in an effort to better
understand the origins of the questions at the core of this thesis, it is
important to look at an interesting and somewhat unique workflow management
system: The Eclipse Integrated Computational Environment.

